[
["index.html", "NCEAS Data Team Training Chapter 1 Welcome to NCEAS! 1.1 First day to-dos 1.2 Account information 1.3 NCEAS Events", " NCEAS Data Team Training Jeanette Clark, Jesse Goldstein, Dominic Mullen, Bryce Mecum, Matt Jones, Peter Slaughter, Irene Steves, Mitchell Maier 2018-04-12 Chapter 1 Welcome to NCEAS! 1.1 First day to-dos Get a tour of the office from Ginger, Jeanette, or Jesse Fill out required paperwork from Michelle Have Ana take your picture (room 309) Set up the remainder of your accounts 1.2 Account information LDAP - NCEAS - Jeanette or Jesse should have set this up prior to your start date to help get other accounts set up by the first day. This account and password control your access to: arcticdata RT queue GitHub - arctic-data and sasap-data Datateam server - follow instructions in email from Nick at NCEAS to reset your datateam password in the terminal ORCiD - create an account NCEAS Slack - get an invite from slack.nceas.ucsb.edu Arctic Data Center Team - after creation of ORCiD and sign-in to both arcticdata.io and test.arcticdata.io, request an add from Chris on Slack Schedule - fill out anticipated quarterly schedule 1.3 NCEAS Events NCEAS hosts a number of events that you are encouraged to attend. Keep an eye on your email but the weekly events are : Roundtable weekly presentation and discussion of research by a visiting or local scientist Wednesdays at 12:15 in the lounge Coffee Klatch coffee, socializing, and news updates for NCEAS Tuesdays at 10:30 in the lounge Salad Potluck potluck salad and socializing, bring a topping or side to share! second Tuesday of the month, 12:15 in the lounge "],
["introduction-to-open-science.html", "Chapter 2 Introduction to open science 2.1 Open science background reading 2.2 Effective data management 2.3 Using DataONE 2.4 Working on a remote server 2.5 Cyberduck instructions 2.6 A note on paths 2.7 A note on scripts 2.8 Exercise 1", " Chapter 2 Introduction to open science These materials are meant to introduce you to the principles of open science, effective data management, and data archival with the DataONE data repository. 2.1 Open science background reading Read the content on the Arctic Data Center (ADC) webpage to learn more about data submission, preservation, and the history of the ADC. We encourage you to follow the links within these pages to gain a deeper understanding. about submission preservation history 2.2 Effective data management Read Matt Jones’ paper on effective data management to learn how we will be organizing datasets prior to archival. 2.3 Using DataONE The Data Observation Network for Earth (DataONE) is a community driven project providing access to data across multiple member repositories, supporting enhanced search and discovery of Earth and environmental data. Watch the first 38 minutes of this video explaining how DataONE works. This video is pretty technical, and you may not understand it all at first. Please feel free to ask Jesse or Jeanette questions. 2.4 Working on a remote server All of the work that we do at NCEAS is done on our remote server, datateam.nceas.ucsb.edu. If you have never worked on a remote server before, you can think of it like working on a different computer via the internet. We access RStudio for our server through this link. To transfer files on and off of the server, you’ll need to use either bash commands in the terminal, or an FTP client. We use a client called Cyberduck. 2.5 Cyberduck instructions To use Cyberduck to transfer local files onto the Datateam server: Open Cyberduck. Check that you have the latest version (Cyberduck -&gt; Check for Update…). If not, download and install the latest (you may need Jesse or Jeanette to enter a pw). Click “Open Connection”. From the drop-down, choose “SFTP (Secure File Transfer Protocol)”. Enter “datateam.nceas.ucsb.edu” for Server. Enter your username and password. Connect. From here, you can drag and drop files to and from the server. 2.6 A note on paths On the servers, paths to files in your folder always start with /home/yourusername/.... When you write scripts, try to avoid writing relative paths (which rely on what you have set your working directory to) as much as possible. Instead, write out the entire path as shown above, so that if another data team member needs to run your script, it is not dependent on a working directory. 2.7 A note on scripts To make it easy to follow the flow of your work, it may help to number your scripts. For example, 01_clean_data.R, 02_edit_EML.R, 03_publish.R. Check out Jenny Bryan’s slidedeck on Naming things for more on this. 2.8 Exercise 1 Download the csv of Table 1 from this paper. Reformat the table using R under the guidelines in the journal article on effective data management. If you need an R refresher, take as much time as you need to go over the data carpentry guide.. You may also find the data carpentry lesson on dplyr and OHI’s data wrangling chapters helpful. Go to test.arcticdata.io and submit your reformatted file with appropriate metadata that you derive from the text of the paper: list yourself as the first ‘Creator’ so your test submission can easily be found, for the purposes of this training exercise, not every single author needs to be listed with full contact details, listing the first two authors is fine, directly copying and pasting sections from the paper (abstract, methods, etc.) is also fine, attributes (column names) should be defined, including correct units and missing value codes. "],
["create-data-packages-in-r.html", "Chapter 3 Create data packages in R 3.1 What is in a package? 3.2 About identifiers 3.3 Uploading packages using R 3.4 Set DataONE nodes 3.5 Publish an object 3.6 Create a resource map 3.7 Exercise 2", " Chapter 3 Create data packages in R This chapter will teach you how to create and submit data package to a DataONE node in R. 3.1 What is in a package? A data package generally consists of at least 3 “objects” or files. Metadata: One object is the metadata file itself. In case you are unfamiliar with metadata, metadata is information that describes data (e.g. who made the data, how was the data made, etc.). The metadata file will be in an XML format, and have the extension .xml (extensible markup language). We often refer to this file as the EML (Ecological Metadata Language), which is the metadata standard that it uses. Data: Other objects in a package are the data files themselves. Most commonly these are data tables (.csv), but they can also be audio files, netCDF files, plain text files, pdf documents, image files, etc. Resource Map: The final object is the resource map. This object is a text file that defines the relationships between all of the other objects in the data package. It says things like “this metadata file describes this data file,” and is critical to making a data package render correctly on the website with the metadata file and all of the data files together in the correct place. Fortunately, we rarely, if ever, have to actually look at the contents of resource maps; they are generated for us using tools in R. 3.2 About identifiers Each object (metadata files, data files, resource maps) on the ADC or KNB (another repo) has a unique identifier, also sometimes called a “PID” (persistent identifier). When you look at the landing page for a data set, for example here, you can find the resource map identifier in the URL (resource_map_doi:10.18739/A2836Z), the metadata identifier in the “General &gt; Identifier” section of the metadata record (doi:10.18739/A2836Z), and the data identifier by clicking the “more info” link next to the data object, and looking at the “Online Distribution Info” section (arctic-data.9546.1). Different versions of a package are linked together by what we call the “version chain” or “obsolescence chain”. Making an update to a data package, such as replacing a data file, changing a metadata record, etc, will result in a new identifier for the new version of the updated object. When making changes to a package, always use the arcticdatautils::update_object or arcticdatautils::publish_update commands on the latest versions of all objects to ensure that the version chain is maintained. 3.3 Uploading packages using R We will be using R to connect to the NSF Arctic Data Center (ADC) data repository to push and pull edits. To identify yourself as an admin you will need to pass a ‘token’ into R. Do this by signing in to the ADC with your ORCiD, hovering over your name and clicking on “My profile”, then navigating to “Settings” and “Authentication Token”, copying the “Token for DataONE R”, and pasting and running it in your R console. This token is your identity on these sites, please treat it as you would a password (ie. don’t paste into scripts that will be shared). The easiest way to do this is to always run the token in the console. There’s no need to keep it in your script since it’s temporary anyway. Sometimes you’ll see a placeholder in scripts to remind users to get their token, such as: options(dataone_test_token = &quot;...&quot;) Next, please be sure these packages are loaded: library(devtools) library(dataone) library(datapack) library(EML) library(XML) If any package could not be loaded, use the following command (replacing package_name with the actual package name) to install the package. install.package(&quot;package_name&quot;) Now install a couple of packages: devtools::install_github(&quot;nceas/arcticdatautils&quot;); library(arcticdatautils) devtools::install_github(&quot;nceas/datamgmt&quot;); library(datamgmt) 3.4 Set DataONE nodes Tell R which repo you want to work with by setting the Coordinating Node (cn) and Member Node (mn) using the appropriate code below. A note on nodes - be very careful about what you publish on production nodes (PROD, or arcticdata.io). These nodes should NEVER be used to publish test or training datasets. 3.4.1 Test nodes # ADC (test.arcticdata.io) cn &lt;- dataone::CNode(&#39;STAGING&#39;) mn &lt;- dataone::getMNode(cn,&#39;urn:node:mnTestARCTIC&#39;) # KNB (dev.nceas.ucsb.edu) cn &lt;- dataone::CNode(&quot;STAGING2&quot;) mn &lt;- dataone::getMNode(cn, &quot;urn:node:mnTestKNB&quot;) 3.4.2 Production nodes # ADC (arcticdata.io) cn &lt;- dataone::CNode(&#39;PROD&#39;) mn &lt;- dataone::getMNode(cn,&#39;urn:node:ARCTIC&#39;) # KNB (knb.ecoinformatics.org) cn &lt;- dataone::CNode(&quot;PROD&quot;) mn &lt;- dataone::getMNode(cn, &quot;urn:node:KNB&quot;) More DataONE nodes can be found here. For now set the node to the test Arctic node. cn &lt;- dataone::CNode(&#39;STAGING&#39;) mn &lt;- dataone::getMNode(cn,&#39;urn:node:mnTestARCTIC&#39;) 3.5 Publish an object Objects (data files, xml metadata files) can be published to a DataONE node using the function arcticdatautils::publish_object. To publish an object, you must first get the format_id of the object you want to publish. A few common format_ids are listed below. # .csv file format_id &lt;- &quot;text/csv&quot; # .txt file format_id &lt;- &quot;text/plain&quot; # metadata file format_id = format_eml() Most objects have format_ids that can be found here on the DataONE website. Metadata files (as shown above) use a special function to set the format_id. If the format_id is not listed at the DataONE website, you can set the format_id &lt;- NULL. Once you know the format_id you can publish your object using the commands below. path &lt;- &quot;path/to/your/file&quot; format_id &lt;- &quot;your/format_id&quot; pid &lt;- arcticdatautils::publish_object(mn, path = path, format_id = format_id) After publishing the object, the pid will need to be added to a resource_map by updating or creating a resource_map. Additionally, the rights and access for the object must be set. 3.6 Create a resource map If you are creating a new data package. You must create a resource_map. Resource_maps provide information about the resources in the data package (e.g. what data files should be included in the package, where is the metadata, etc.) To create a new resource map with an existing published metadata_pid and data_pids, use the following command. resource_map_pid &lt;- arcticdatautils::create_resource_map(mn, metadata_pid = metadata_pid, data_pids = data_pids) 3.7 Exercise 2 Locate the data package you published in Exercise 1 by navigating to the “My Profile” section on test.arcticdata.io. Download the metadata and data files and transfer them to the Datateam server. Using the functions described in the section above, publish your metadata record and data file to the site. (When you do so, make sure you save the pids to different variables in R…) i.e. data_pid &lt;- arcticdatautils::publish_object(...) metadata_pid &lt;- arcticdatautils::publish_object(...) Then create a resource_map containing your data and metadata. View your new data set (which is identical to the one you created previously) by appending the metadata pid the end of the URL test.arcticdata.io/#view/… "],
["update-a-data-package.html", "Chapter 4 Update a data package 4.1 Get package and EML 4.2 Update an object 4.3 Update a package 4.4 Exercise 3", " Chapter 4 Update a data package This chapter will teach you how to edit an existing data package in R. 4.1 Get package and EML To update a package, you must load it into your R environment. After setting your node, use the following commands to load the package. rm_pid &lt;- &quot;your_resource_map_pid&quot; pkg &lt;- arcticdatautils::get_package(mn, rm_pid, file_names = TRUE) After loading the package, you can also load the eml file into R using the following command. eml &lt;- EML::read_eml(rawToChar(dataone::getObject(mn, pkg$metadata))) Tip to always have the most recent resource map. When editing resource_maps, you always want to be working with the most recent update. To ensure you have the most recent resource_map, you can use the following commands. rm_pid_original &lt;- &quot;your_original_resource_map_pid&quot; all_rm_versions &lt;- arcticdatautils::get_all_versions(mn, rm_pid_original) rm_pid &lt;- all_rm_versions[length(all_rm_versions)] 4.2 Update an object To update an object (a data file), use arcticdatautils::update_object. First make sure you have the package that contains the data you want to update, loaded into R. Then define the path to the updated object. data_path &lt;- &quot;path/to/data/file.csv&quot; Then update the object at the Member Node. When updating the object, it is best practice to update your pkg in R at the same time. In the following code, replace i with the index of the data pid you want to update. pkg$data[i] &lt;- update_object(mn, pid &lt;- pkg$data[i], path &lt;- data_path, format_id = NULL, new_pid = NULL, sid = NULL) Note that you will need to be explicit about your format_id here based on the file type. A list of format IDs can be found here on the DataONE website. 4.3 Update a package To update a package, use arcticdatautils::publish_update. This function has an argument for adding data PIDs (or otherwise including existing data PIDs to make sure that they stay with the package). This function allows you to add or remove data objects, and/or make metadata edits. First make sure you have the package you want to update, loaded into R. 4.3.1 Updating metadata If you need to make edits to the metadata, do so. Then write the eml metadata to a file. eml_path &lt;- &quot;path/to/metadata/science_metadata.xml&quot; write_eml(eml, eml_path) Next verify the eml is valid. EML::eml_validate(eml_path) The above should return TRUE if the eml is valid. If it returns FALSE, fix the eml file, use write_eml() to save the fixed file, and retry. 4.3.2 Updating data If you need to make edits to the data, do so. 4.3.3 Publish update Finally, update your data package at the Member Node. update &lt;- arcticdatautils::publish_update(mn, metadata_pid = pkg$metadata, resource_map_pid =pkg$resource_map, data_pids = pkg$data, child_pids = NULL, use_doi = FALSE, metadata_path = eml_path, public = FALSE, check_first = TRUE) If a package is ready to be public, you can change the public argument in the publish_update call to TRUE. If you want to publish with a DOI (Digital Object Identifier) instead of a UUID (Universally Unique Identifier), you can change the use_doi argument to TRUE. This should only be done after the package is finalized and has been thoroughly reviewed! If the package has children change child_pids to pkg$child_packages. 4.4 Exercise 3 Locate the data package you published in the previous exercise by navigating to the URL test.arcticdata.io/#view/… Load the package and eml into R using the above commands. Load the data file associated with the package into R as a data.frame. (Hint: use read.csv() to upload the data file from your computer/the server) Make an edit to the data in R (e.g. change one of the colnames to &quot;TEST&quot;) Save the edited data. (Hint: use write.csv(data, row.names = FALSE)) Update the data file in the package with the edited data file using the above commands. Update your package using the above commands. "],
["explore-eml.html", "Chapter 5 Explore EML", " Chapter 5 Explore EML We use the Ecological Metadata Language (EML) to store structured metadata for all datasets submitted to the Arctic Data Center. EML is written in XML (extensible markup language) and functions for building and editing EML are in the EML R package. For additional background on EML and principles for metadata creation, check out this paper. The first task when editing an eml file is navigating the eml file. An eml file is organized in a structure that contains many lists nested within other lists. The function EML::eml_view() (install.packages(&quot;listviewer&quot;) if it doesn’t work) allows you to get a crude view of an eml file in the viewer. It can be useful for exploring the file. To navigate this complex structure in R, use the @ symbol. The @ symbol allows you to go deeper into the eml structure and to see what slots are nested within other slots. However, you have to tell R where you want to go in the structure when you use the @ symbol. For example, if you want to go into the dataset of your eml you would use the command eml@dataset. If you want to go to the creators of your data set you would use eml@dataset@creator. Note here that creators are contained within dataset. If you aren’t sure where you want to go, hit the tab button on your keyboard after typing @ and a list of available locations in the structure will appear (e.g., eml@&lt;TAB&gt;): RStudio Autocompletion Example Note if you hit tab, and the only option is .Data, this implies most likely that you are trying to go deeper within a list. For example eml@dataset@creator@&lt;TAB&gt; will return only .Data. This is because creator is a list object (i.e. you can have multiple creators). If you want to go deeper into creator, you first must tell R which creator you are interested in. Do this by writing [[i]] first where i is index of the creator you are concerned with. For example, if you want to look at the first creator i = 1. Now eml@dataset@creator[[1]]@&lt;TAB&gt; will give you many more options. Note, .Data also sometimes means you have reached the end of a branch in the eml structure. 5.0.1 Understand the EML schema Another great resource for navigating the eml structure is looking at the schema which defines the structure. The .png files on this page show the schema as a diagram. Additional information on the schema and how different elements are defined can be found here). However, the schema is complicated and may take some time to get familiar with before you will be able to fully understand it. For example, let’s take a look at eml-party. To start off, notice that some elements are in solid boxes, whereas others are in dashed boxes. A solid box indicates that the element is required if the element above it (to the left in the schema) is used, whereas a dashed box indicates that the element is optional. Notice also that below the givenName element it says “0..infinity”. This means that the element is unbounded — a single party can have many given names and there is no limit on how many you can add. However, this text does not appear for the surName element — a party can have only one surname. You will also see icons linking the eml slots together which indicate the ordering of subsequent slots. These can indicate either a “sequence” or a “choice”. In our example from eml-party, a “choice” icon indicates that either an individualName, organizationName, or positionName is required, but you do not need all three. However, the “sequence” icon tells us that if you use an individualName, you must include the surName as a child element. If you include the optional child elements salutation and givenName, they must be written in the order presented in the schema. The eml schema sections you may find particularly helpful include eml-party, eml-attribute, and eml-physical. For a more detailed description of the eml schema, see the reference section on exploring EML. 5.0.2 Access specific elements The eml_get function is a powerful tool for exploring EML (more on that here). It takes any chunk of eml and returns all instances of the element you specify. Note: you’ll have to specify the element of interest exactly, according to the spelling/capitalization conventions used in EML. Here are some examples: library(EML) eml &lt;- EML::read_eml(system.file(&quot;example-eml.xml&quot;, package = &quot;arcticdatautils&quot;)) EML::eml_get(eml, &quot;creator&quot;) ## [[1]] ## An object of class &quot;ListOfcreator&quot; ## [[1]] ## &lt;creator system=&quot;uuid&quot;&gt; ## &lt;individualName&gt; ## &lt;givenName&gt;Bryce&lt;/givenName&gt; ## &lt;surName&gt;Mecum&lt;/surName&gt; ## &lt;/individualName&gt; ## &lt;organizationName&gt;National Center for Ecological Analysis and Synthesis&lt;/organizationName&gt; ## &lt;/creator&gt; EML::eml_get(eml, &quot;boundingCoordinates&quot;) ## &lt;boundingCoordinates&gt; ## &lt;westBoundingCoordinate&gt;-135&lt;/westBoundingCoordinate&gt; ## &lt;eastBoundingCoordinate&gt;-134&lt;/eastBoundingCoordinate&gt; ## &lt;northBoundingCoordinate&gt;59&lt;/northBoundingCoordinate&gt; ## &lt;southBoundingCoordinate&gt;57&lt;/southBoundingCoordinate&gt; ## &lt;/boundingCoordinates&gt; EML::eml_get(eml, &quot;url&quot;) ## [1] &quot;ecogrid://knb/urn:uuid:89bec5d0-26db-48ac-ae54-e1b4c999c456&quot; You can also use the which_in_eml function from the datamgmt package to get indices within an eml list. Here are some examples: # Question: Which creators have a surName &quot;Smith&quot;? n &lt;- which_in_eml(eml@dataset@creator, &quot;surName&quot;, &quot;Smith&quot;) # Answer: eml@dataset@creator[n] # Question: Which dataTables have an entityName that begins with &quot;2016&quot; n &lt;- which_in_eml(eml@dataset@dataTable, &quot;entityName&quot;, function(x) {grepl(&quot;^2016&quot;, x)}) # Answer: eml@dataset@dataTable[n] # Question: Which attributes in dataTable[[1]] have a numberType &quot;natural&quot;? n &lt;- which_in_eml(eml@dataset@dataTable[[1]]@attributeList@attribute, &quot;numberType&quot;, &quot;natural&quot;) # Answer: eml@dataset@dataTable[[1]]@attributeList@attribute[n] #&#39; # Question: Which dataTables have at least one attribute with a numberType &quot;natural&quot;? n &lt;- which_in_eml(eml@dataset@dataTable, &quot;numberType&quot;, function(x) {&quot;natural&quot; %in% x}) # Answer: eml@dataset@dataTable[n] "],
["editing-eml-in-r.html", "Chapter 6 Editing EML in R 6.1 Edit an EML element 6.2 Edit attributeLists 6.3 Set physical 6.4 Edit dataTables 6.5 Edit otherEntities 6.6 Set coverages 6.7 Set methods 6.8 Set parties 6.9 Exercise 4", " Chapter 6 Editing EML in R This chapter is a practical tutorial for using R to read, edit, write, and validate EML documents. Much of the information here can also be found in the vignettes for the R packages used in this section (e.g. the EML package). Once you have an eml file loaded into R, you can use R to edit the eml file. 6.1 Edit an EML element There are multiple ways to edit an EML element. 6.1.1 Edit EML with strings The most basic way to edit an EML element would be to go into the eml schema to the location of a character string and then replace that character string with a new character string. For example to change the title one could use the following commands. new_title &lt;- &quot;New Title&quot; eml@dataset@title[[1]]@.Data &lt;- new_title However, this isn’t the best method to edit the EML unless you are an expert both in S4 objects and in the EML schema, since the nesting and lists of elements can get very complex. 6.1.2 Edit EML with EML For editing simple text sections, a better option is to use EML::read_eml. To use this function, one would first run the slot that is in need of editing. For example, for a title this would involve calling eml@dataset@title[[1]] and then copying the result. In this case, the result will be of the form &lt;title&gt;Title Text Here&lt;/title&gt;. To make a new title, one would replace the text between the &lt;title&gt;&lt;/title&gt; tags using a similar workflow as below. new_title &lt;- EML::read_eml(&quot;&lt;title&gt;New Title 2&lt;/title&gt;&quot;) eml@dataset@title[[1]] &lt;- new_title Note, without specifying which title with [[1]], the following code will give you the error Error in (function (cl, name, valueClass) : assignment of an object of class “title” is not valid for @‘title’ in an object of class “dataset”; is(value, &quot;ListOftitle&quot;) is not TRUE. # Bad Example new_title &lt;- EML::read_eml(&quot;&lt;title&gt;New Title 2&lt;/title&gt;&quot;) eml@dataset@title &lt;- new_title The above gives an error because eml@dataset@title is a slot for a list and new_title is a single object. Therefore you must either specify which title you want to replace as was done above by specifying the first title in the list with [[1]] or turn new_title into a list/vector utilizing the c() command as follows. new_title &lt;- EML::read_eml(&quot;&lt;title&gt;New Title 2&lt;/title&gt;&quot;) eml@dataset@title &lt;- c(new_title) However, if there were multiple titles, the above would replace all the titles with the single title. This behavior may or may not be desirable so be careful. One final note, a benefit of this method to edit EML objects is that advanced text features can be easily added using this workflow. 6.1.3 Edit EML with objects A final way to edit an EML element would be to build a new object to replace the old object. To begin, you must determine the class of the object you want to edit (generally this is just the schema name of the object). The function class() is helpful here. For example, if you want to edit eml@dataset@title[[1]] use the following command to find the class. class(eml@dataset@title[[1]]) The result shows that this object has a class title. Therefore you must replace it with an object of class title. To do so use as(). To use as() input the the desired string followed by the desired class. new_title &lt;- as(&quot;New Title 3&quot;, &quot;title&quot;) eml@dataset@title[[1]] &lt;- new_title #or eml@dataset@title &lt;- c(new_title) Note, if you want to create an object with nested objects, you may have to use the command new() which is similar to as() but with the order of specifying values and class switched. See help on editing datatables for an example of when to use new(). 6.2 Edit attributeLists Attributes are stored in an attributeList. When editing attributes in R, you need to create one-three objects: A data.frame of attributes A data.frame of custom units (if applicable) A data.frame of factors (if applicable) Note, to edit/examine an existing attribute table already in a eml file you can use the following commands. attributeList &lt;- EML::get_attributes(eml@dataset@dataTable[[i]]@attributeList) attributes &lt;- attributeList$attributes 6.2.1 Edit Attributes Attribute information should be stored in a data.frame with the following columns. attributeName: The name of the attribute as listed in the csv. Required. e.g.: “c_temp” attributeLabel: A descriptive label that can be used to display the name of an attribute. It is not constrained by system limitations on length or special characters. Optional. e.g.: “Temperature (Celsius)” attributeDefinition: Longer description of the attribute, including the required context for interpreting the attributeName. Required. e.g.: “The near shore water temperature in the upper inter-tidal zone, measured in degrees Celsius.” measurementScale: One of: nominal, ordinal, dateTime, ratio, interval. Required. nominal: unordered categories or text. e.g.: (Male, Female) or (Yukon River, Kuskokwim River) ordinal: ordered categories. e.g.: Low, Medium, High dateTime: date or time values from the Gregorian calendar. e.g.: 01-01-2001 ratio: measurement scale with a meaningful zero point in nature. Ratios are proportional to the measured variable. e.g.: 0 Kelvin represents a complete absence of heat. 200 Kelvin is half as hot as 400 Kelvin. 1.2 meters per second is twice as fast as 0.6 meters per second. interval: values from a scale with equidistant points, where the zero point is arbitrary. This is usually reserved for degrees Celsius or Fahrenheit, or latitude and longitude coordinates, or any other human-constructed scale. e.g.: there is still heat at 0° Celsius; 12° Celsius is NOT half as hot as 24° Celsius domain: One of: textDomain, enumeratedDomain, numericDomain, dateTimeDomain. Required. textDomain: text that is free-form, or matches a pattern enumeratedDomain: text that belongs to a defined list of codes and definitions. e.g.: CASC = Cascade Lake, HEAR = Heart Lake dateTimeDomain: dateTime attributes numericDomain: attributes that are numbers (either ratio or interval) formatString: Required for dateTimeDomain, NA otherwise. Format string for dates, e.g. “MM/DD/YYYY”. definition: Required for textDomain, NA otherwise. Definition for attributes that are a character string, matches attribute definition in most cases. unit: Required for numericDomain, NA otherwise. Unit string. If the unit is not a standard unit, a warning will appear when you create the attribute list, saying that it has been forced into a custom unit. Use caution here to make sure the unit really needs to be a custom unit. A list of standard units can be found here: https://knb.ecoinformatics.org/#external//emlparser/docs/eml-2.1.1/./eml-unitTypeDefinitions.html#StandardUnitDictionary numberType: Required for numericDomain, NA otherwise. Options are “real”, “natural”, “whole”, “integer”. real: positive and negative fractions and non fractions (…-1,-0.25,0,0.25,1…) natural: non-zero positive counting numbers (1,2,3…) whole: positive counting numbers and zero (0,1,2,3…) integer: positive and negative counting numbers and zero (…-2,-1,0,1,2…) missingValueCode: Code for missing values (e.g.: ‘-999’, ‘NA’, ‘NaN’). NA otherwise. Note that an NA missing value code should be a string, ‘NA’, and numbers should also be strings, ‘-999.’ missingValueCodeExplanation: Explanation for missing values, NA if no missing value code exists. You can create attributes by hand by typing them out in R following a workflow similar to that as below. attributes &lt;- data.frame( attributeName = c(&#39;Date&#39;, &#39;Location&#39;, &#39;Region&#39;,&#39;Sample_No&#39;, &#39;Sample_vol&#39;, &#39;Salinity&#39;, &#39;Temperature&#39;, &#39;sampling_comments&#39;), attributeDefinition = c(&#39;Date sample was taken on&#39;, &#39;Location code representing location where sample was taken&#39;,&#39;Region where sample was taken&#39;, &#39;Sample number&#39;, &#39;Sample volume&#39;, &#39;Salinity of sample in PSU&#39;, &#39;Temperature of sample&#39;, &#39;comments about sampling process&#39;), measurementScale = c(&#39;dateTime&#39;, &#39;nominal&#39;,&#39;nominal&#39;, &#39;nominal&#39;, &#39;ratio&#39;, &#39;ratio&#39;, &#39;interval&#39;, &#39;nominal&#39;), domain = c(&#39;dateTimeDomain&#39;, &#39;enumeratedDomain&#39;,&#39;enumeratedDomain&#39;, &#39;textDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;numericDomain&#39;, &#39;textDomain&#39;), formatString = c(&#39;MM-DD-YYYY&#39;, NA,NA,NA,NA,NA,NA,NA), definition = c(NA,NA,NA,&#39;Sample number&#39;, NA, NA, NA, &#39;comments about sampling process&#39;), unit = c(NA, NA, NA, NA,&#39;milliliter&#39;, &#39;dimensionless&#39;, &#39;celsius&#39;, NA), numberType = c(NA, NA, NA,NA, &#39;real&#39;, &#39;real&#39;, &#39;real&#39;, NA), missingValueCode = c(NA, NA, NA,NA, NA, NA, NA, &#39;NA&#39;), missingValueCodeExplanation = c(NA, NA, NA,NA, NA, NA, NA, &#39;no sampling comments&#39;), stringsAsFactors = FALSE) However, typing this out in R can be a major pain. Luckily, there is a shiny app that you can use to build attribute information. You can use the app to build attributes from a data file loaded into R (recommended as the app will auto-fill some fields for you), or to edit an existing attribute table, or create attributes from scratch using the following commands (the commands will launch a shiny app in your web browser). # From data (recommended) datamgmt::create_attributes_table(data = data) # From an attribute table datamgmt::create_attributes_table(attributes_table = attributes_table) # From scratch datamgmt::create_attributes_table() Once you are done editing a table in the app, click the Print button to print text of a code that will build a data frame in R. Copy that code and assign it to a variable in your script (e.g. attributes &lt;- data.frame(...)). 6.2.2 Edit Custom Units If you have units that are not in the standard EML unit list, you will need to build a custom unit list. To determine if your units are standard or not you can look through the standard units by using the following commands. standardUnits &lt;- get_unitList() View(standardUnits$units) Additionally, datamgmt::create_attributes_table will tell you if each of your units are standard or not. If your unit is not standard, you should use the following code to help auto-generate a custom unit. datamgmt::get_custom_units(&quot;your_unit&quot;) Note, datamgmt::create_attributes_table calls datamgmt::get_custom_units for you! Custom units need to have the following columns. id unitType parentSI multiplierToSI abbreviation definition datamgmt::get_custom_units will auto-generate many of these fields for you (but don’t just assume the autogeneration will be perfect, always ensure the autogeneration correctly handles your unit) 6.2.3 Edit Factors For attributes that are enumeratedDomains, a table is needed with three columns: attributeName, code, and definition. attributeName should be the same as the attributeName within the attribute table and repeated for all codes belonging to a common attribute. code should contain all unique values of the given attributeName that exist within the actual data. definition should contain a plain text definition that describes each code. There is a tab in the datamgmt::create_attributes_table app that will help you build factors. If you need to build factors by hand, you can use named character vectors and then convert them to a data frame as shown in the example below. In this example, there are two enumerated domains in the attribute list - “Location” and “Region” Location &lt;- c(CASC = &#39;Cascade Lake&#39;, CHIK = &#39;Chikumunik Lake&#39;, HEAR = &#39;Heart Lake&#39;, NISH = &#39;Nishlik Lake&#39; ) Region &lt;- c(W_MTN = &#39;West region, locations West of Eagle Mountain&#39;, E_MTN = &#39;East region, locations East of Eagle Mountain&#39;) The definitions are then written into a data frame using the names of the named character vectors, and their definitions. factors &lt;- rbind(data.frame(attributeName = &#39;Location&#39;, code = names(Location), definition = unname(Location)), data.frame(attributeName = &#39;Region&#39;, code = names(Region), definition = unname(Region))) 6.2.4 Finalize attributeList Once you have built your attributes, factors, and custom units, you can add them to eml objects. Attributes and factors are combined to form an attributeList using the following command. attributeList &lt;- EML::set_attributes(attributes = attributes, factors = factors) This attributeList must then be added to a dataTable. Custom units are added to additionalMetadata using the following command. unitlist &lt;- set_unitList(custom_units) eml@additionalMetadata &lt;- c(as(unitlist, &quot;additionalMetadata&quot;)) 6.3 Set physical To set the physical aspects of a dataTable, use the following commands to build a physical object from a data pid that exists in your package. physical &lt;- arcticdatautils::pid_to_eml_physical(mn, pkg$data[[i]]) Alternatively, you can get the physical of a data object not in your package by simply inputting the data pid. physical &lt;- arcticdatautils::pid_to_eml_physical(mn, &quot;your_data_pid&quot;) Note, the physical must then be added to a dataTable. A final, but not recommended option, is to set the physical by hand. To do so one can use a workflow similar to the one below. However, the far superior workflow is to publish or update your data first and then use `pid_to_eml_physical to set the physical id &lt;- &#39;your_data_pid&#39; #this should be an actual PID path &lt;- &#39;~/your/data/path&#39; #path to data table physical &lt;- EML::set_physical(objectName = &#39;your_file_name&#39;, id = id, size = as.character(file.size(path)), sizeUnit = &#39;bytes&#39;, authentication = digest(path, algo=&quot;sha1&quot;, serialize=FALSE, file=TRUE), authMethod = &#39;SHA-1&#39;, numHeaderLines = &#39;1&#39;, fieldDelimiter = &#39;,&#39;, url = paste0(&#39;https://cn.dataone.org/cn/v2/resolve/&#39;, id)) 6.4 Edit dataTables To edit a dataTable, first edit/create an attributeList and set the physical. Then create a new dataTable with the new() command as follows. dataTable &lt;- new(&quot;dataTable&quot;, entityName = &quot;A descriptive name for the data (does not need to be the same as the data file)&quot;, entityDescription = &quot;A description of the data&quot;, physical = physical, attributeList = attributeList) The dataTable must then be set to the eml i.e. eml@dataset@dataTable[[i]] &lt;- dataTable. 6.5 Edit otherEntities 6.5.1 Remove otherEntities To remove an otherEntity use the following command. This may be useful if a data object is originally listed as an otherEntity and then transferred to a dataTable. eml@dataset@otherEntity[[i]] &lt;- NULL 6.5.2 Create otherEntities If you need to create/update an otherEntitymake sure to publish or update your data object first (if it is not already on the DataONE node). Then build your otherEntity. otherEntity &lt;- arcticdatautils::pid_to_eml_other_entity(mn, pkg$data[[i]])[[1]] Alternatively, you can build the otherEntity of a data object not in your package by simply inputting the data pid. otherEntity &lt;- arcticdatautils::pid_to_eml_other_entity(mn, &quot;your_data_pid&quot;)[[1]] Next, give the otherEntity a name and description. otherEntity@entityName &lt;- as(&quot;A descriptive name for the data&quot;, &quot;entityName&quot;) otherEntity@entityDescription &lt;- as(&quot;A description of the data&quot;, &quot;entityDescription&quot;) The otherEntity must then be set to the eml i.e. eml@dataset@otherEntity[[i]] &lt;- otherEntity 6.6 Set coverages Sometimes EML documents may lack coverage information describing the temporal, geographic, or taxonomic coverage of a data set. This example shows how to create coverage information from scratch, or replace an existing coverage element with an updated one. You can view the current coverage (if it exists) by entering eml@dataset@coverage into the console. Here the coverage, including temporal, taxonomic, and geographic coverages, is defined using set_coverage. coverage &lt;- EML::set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;), geographicDescription = &quot;The geographic region covers the lake region near Eagle Mountain.&quot;, west = -154.6192, east = -154.5753, north = 68.3831, south = 68.3619) eml@dataset@coverage &lt;- coverage You can also set multiple geographic (or temporal) coverages. Here is an example of how you might set two geographic coverages. geocov1 &lt;- new(&quot;geographicCoverage&quot;, geographicDescription = &quot;The geographich region covers area 1&quot;, boundingCoordinates = new(&quot;boundingCoordinates&quot;, northBoundingCoordinate = new(&quot;northBoundingCoordinate&quot;, 68), eastBoundingCoordinate = new(&quot;eastBoundingCoordinate&quot;, -154), southBoundingCoordinate = new(&quot;southBoundingCoordinate&quot;, 67), westBoundingCoordinate = new(&quot;westBoundingCoordinate&quot;, -155))) geocov2 &lt;- new(&quot;geographicCoverage&quot;, geographicDescription = &quot;The geographich region covers area 2&quot;, boundingCoordinates = new(&quot;boundingCoordinates&quot;, northBoundingCoordinate = new(&quot;northBoundingCoordinate&quot;, 65), eastBoundingCoordinate = new(&quot;eastBoundingCoordinate&quot;, -155), southBoundingCoordinate = new(&quot;southBoundingCoordinate&quot;, 64), westBoundingCoordinate = new(&quot;westBoundingCoordinate&quot;, -156))) coverage &lt;- EML::set_coverage(beginDate = &#39;2012-01-01&#39;, endDate = &#39;2012-01-10&#39;, sci_names = c(&#39;exampleGenus exampleSpecies1&#39;, &#39;exampleGenus ExampleSpecies2&#39;)) eml@dataset@coverage@geographicCoverage &lt;- c(geocov1, geocov2) 6.7 Set methods The methods tree in the EML section has many different options, visible in the schema. You can create new elements in the methods tree by following the schema and using the “new” command. Remember you can explore possible slots within an element by creating an empty object of the class you are trying to create. For example, method_step &lt;- new('methodStep'), and using auto-complete on method_step@. Potentially the most useful way to set methods is by editing with EML Another simple, and potentially useful way to add methods to an EML that have no methods at all is adding them via a word document. An example is shown below: methods1 &lt;- set_methods(&#39;methods_doc.docx&#39;) eml@dataset@methods &lt;- methods1 If you want to make minor changes to existing method information that has a lot of nested elements, your best bet may be to edit the EML manually in a text editor (or in RStudio), otherwise there is a risk of accidentally overwriting nested elements with blank object classes, therefore losing method information. 6.8 Set parties To add people, with their addresses, you need to add addresses as their own object class, which you then add to the contact, creator, or associated party classes. NCEASadd &lt;- new(&quot;address&quot;, deliveryPoint = &quot;735 State St #300&quot;, city = &quot;Santa Barbara&quot;, administrativeArea = &#39;CA&#39;, postalCode = &#39;93101&#39;) The creator, contact, and associated party classes can easily be created using functions from the arcticdatautils package. Here, we use eml_creator to set our data set creator. JC_creator &lt;- arcticdatautils::eml_creator(&quot;Jeanette&quot;, &quot;Clark&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) eml@dataset@creator &lt;- c(JC_creator) Similarly, we can set the contacts. In this case, there are two, so we set eml@dataset@contact as a ListOfcontact, which contains both of them. JC_contact &lt;- arcticdatautils::eml_contact(&quot;Jeanette&quot;, &quot;Clark&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) JG_contact &lt;- arcticdatautils::eml_contact(&quot;Jesse&quot;, &quot;Goldstein&quot;, &quot;NCEAS&quot;, &quot;jgoldstein@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, address = NCEASadd) eml@dataset@contact &lt;- c(JC_contact, JG_contact) Finally, the associated parties are set. Note that associated parties MUST have a role defined, unlike creator or contact. JG_ap &lt;- arcticdatautils::eml_associated_party(&quot;Jesse&quot;, &quot;Goldstein&quot;, &quot;NCEAS&quot;, &quot;jclark@nceas.ucsb.edu&quot;, phone = &quot;123-456-7890&quot;, address = NCEASadd, userId = &quot;https://orcid.org/WWWW-XXXX-YYYY-ZZZZ&quot;, role = &quot;metaataProvider&quot;) eml@dataset@associatedParty &lt;- c(JG_ap) 6.9 Exercise 4 Make sure your package from before is loaded into R Replace the existing dataTable with a new dataTable object with an attribute list and physical section you write in R using the above commands Then write, validate, and update your package Use the checklist to review your submission. Make edits where necessary, and publish updates as needed "],
["system-metadata.html", "Chapter 7 System metadata 7.1 Edit sysmeta 7.2 Set rights and access", " Chapter 7 System metadata Every object on the ADC (or on the KNB (the ‘Knowledge Network for Biocomplexity’)) has “system metadata”. An object’s system metadata has information about the file itself, such as the name of the file, the format, who the rights holder is, and what the access policy is (amongst other things). Sometimes we will need to edit system metadata in order to make sure that things on the webpage display correctly, or to ensure a file downloads from the website with the correct file name and extension. Although the majority of system metadata changes that need to be made are done automatically, sometimes we need to change aspects of the system metadata (or ‘sysmeta’ for short) manually. 7.1 Edit sysmeta To edit the sysmeta of an object with a pid (data, metadata, resource_map, etc.) first load the sysmeta into R using the following command. sysmeta &lt;- dataone::getSystemMetadata(mn, pid) Then edit the sysmeta slots by using @ functionality. For example, to change the fileName use the following command. sysmeta@fileName &lt;- &#39;New File Name.csv&#39; Note that some slots cannot be changed by simple text replace (particularly the accessPolicy). There are various helper functions for changing the accessPolicy and rightsHolder such as datapack::addAccessRule (which takes the sysmeta as an input) or arcticdatautils::set_rights_and_access, which only requires a PID. In general, you most frequently need to use dataone::getSystemMetadata to change either the formatId or fileName slots. After you have changed the necessary slot, you can update the system metadata using the following command. updateSystemMetadata(mn, pid, sysmeta) 7.1.1 Identifiers and sysmeta Importantly, changing the system metadata does NOT necessitate a change in the PID of an object. This is because changes to the system metadata do not change the object itself, they are only changing the description of the object (although ideally the system metadata is accurate when an object is first published). 7.1.2 Additional resources For a more in-depth (and technical) guide to sysmeta, check out the DataOne documentation: System Metadata Data Types in CICore 7.2 Set rights and access One final step when creating/updating packages is to make sure that the rights and access on all the objects that were uploaded are set correctly within the sysmeta. The function arcticdatautils::set_rights_and_access will set both, and arcticdatautils::set_access will just set access. There are two functions for this because a rights holder should always have access, but not all people who need access are rights holders. The rights holder of the data package is typically the submitter (if the dataset is submitted through the web form, sometimes referred to as the “registry”), but if a data team member is publishing objects for a PI, the rights holder should be the main point of contact for the dataset (i.e. the person who requested that we upload the data for them). To set the rights and access for all of the objects in a package, first get the ORCiD of the person you want to give access and rights to. You can set this manually, or grab it from one of the creators in an eml file. # Manually set ORCiD subject &lt;- &#39;http://orcid.org/PUT0-YOUR-ORCD-HERE&#39; # Set ORCiD from eml creator subject &lt;- eml@dataset@creator[[1]]@userId[[1]]@.Data subject &lt;- sub(&quot;^https://&quot;, &quot;http://&quot;, subject) Note, when setting metadata, the ORCiD must start with http://. ORCiDs in eml should start with https://. The sub command above will change this formatting for you. Next, set the rights and access using the following command. set_rights_and_access(mn, pids = c(pkg$metadata, pkg$data, pkg$resource_map), subject = subject, permissions = c(&#39;read&#39;,&#39;write&#39;,&#39;changePermission&#39;)) If you ever need to remove/add public access to your package or object, you can use remove_public_read or set_public_read, respectively. arcticdatautils::remove_public_read(mn, c(pkg$metadata, pkg$data, pkg$resource_map)) 7.2.1 Exercise 5 Read the system metadata in from the data file you uploaded in previously. Check to make sure the fileName and formatId are set correctly. Update the system metadata if necessary. Set the rights and access for all objects with your ORCiD. "],
["using-git-in-rstudio.html", "Chapter 8 Using Git in RStudio 8.1 Introduction 8.2 Setting up git 8.3 Working with the repository 8.4 My Git tab disappeared", " Chapter 8 Using Git in RStudio 8.1 Introduction Important! If you have never used git before, or only used it a little, or have no idea what is is, check out this intro to git put together by the ecodatascience group at UCSB. https://github.com/eco-data-science/github-intro-2/blob/master/index.pdf Don’t worry too much about the forking and branching sections, as we will primarily be using the basic commit-pull-push commands. After you have read through that presentation, come back to this chapter. 8.1.1 So why do I need to use this again? There are several reasons why using the arctic-data GitHub repository is helpful, both for you and for the rest of the data team. Here are a few: Versioning: Accidentally make a change to your code and can’t figure out why it broke? Wish you could go back to that version that worked? If you add your code to the GitHub repo you can do this! Reproducibility: Being able to reproduce how you accomplished something is incredibly important. We should be able to tell anyone exactly how data have been reformatted, how metadata have been altered, and how packages have been created. As a data center, this is especially important for us with data team members that stay for 6-12 months because we may need to go back and figure out how something was done after the intern or fellow who wrote the code left the team. Troubleshooting: If you are building a particularly complicated EML, or doing some other advanced task, it is much easier for Jesse, Jeanette, or Bryce to troubleshoot your code if it is on the GitHub repo. We can view, troubleshoot, and fix bugs very easily when code is on the GitHub repo, with the added bonus of being able to go back a version if something should break. Solve future problems: Some of the issues we see in ADC submissions come up over and over again. When all of our code is on GitHub, we can easily reference code built for other submissions, instead of trying to solve the same problems over and over again from scratch. 8.2 Setting up git Now you need to set up your Git global options, and tell it who you are. At the top of your RStudio window, select Tools &gt; Shell. In the prompt, you will need to run two commands, one at a time. The first tells Git what your name is, the second what your email address is. These are the commands: git config --global user.name &quot;My Name&quot; git config --global user.email myemail@domain.com After running these commands, the shell prompt should look like this: 8.2.1 Cloning the arctic-data repo Next, you need to clone the arctic-data repository to your RStudio. You do this by adding it as a “project”. In your RStudio window, click File &gt; New Project. Then click ‘Version Control’, and then select the ‘Git’ option. If you are prompted to save your workspace during this process, make sure all of your work is saved, and you don’t need anything in your environment, and then click ‘Don’t Save’. You should see a prompt asking you for a URL. Fill it out like this to clone the arctic-data repository into the top level of your home directory. Note that the URL is the same URL you use to view the repository on the web. If you are using the sasap-data repository, the URL is http://github.nceas.ucsb.edu/NCEAS/sasap-data/. clone repo You will be prompted for your username and password, and then Git will clone the directory. The username/password you use should be the same one you use to log in when you go to http://github.nceas.ucsb.edu/KNB/arctic-data. Now you should have a directory called arctic-data in your RStudio files window. 8.3 Working with the repository 8.3.1 Adding a new script If you have been working on a script that you want to put in the arctic-data GitHub repo, you need to save it somewhere in the arctic-data folder you made in your account on the server. You can do this by either moving your script into the folder or using the save-as functionality. Note that Git will try and version anything that you save in this folder, so you should be careful about what you save here. For our purposes, things that probably shouldn’t be saved in this folder include: Tokens: Any token file or script with a token in it should NOT be saved in the repository. Others could steal your login credentials if you put a token in GitHub. Data files: Git does not version data files very well. You shouldn’t save any .csv files or any other data files (including metadata). Workspaces/.RData: If you are in the habit of saving your R workspace, you shouldn’t save it in this directory. Plots/Graphics: For the same reasons as data files Note: Do not EVER make a commit that you don’t understand. If something unexpected (like a file you have never worked on) shows up in your Git tab, ask Jesse or Jeanette before committing. After you save your script in the appropriate place within the arctic-data folder, it will show up in your Git tab looking like this: Before you commit your changes, you need to click the little box under “staged.” Do not stage or commit any .Rproj file. After clicking the box for your file, click “Commit” to commit your changes. In the window that pops up (you may need to force the browser to allow pop-ups), write your commit message. Always include a commit message. Remember that the commit message should be a concise description of the changes being made to a file. Your window should look like this: Push ‘Commit’, and your commit will be saved. Now you want to merge the commits you made with the master version of the repository. You do this by using the command “Push.” But before you push, you always need to pull first to avoid merge conflicts. Click “Pull” and type in your credentials. Then, assuming you don’t have a merge conflict, you can push your changes by clicking “Push”. Always remember, the order is commit-pull-push 8.3.2 Editing a script If you want to change a script, the workflow is the same. Just open the script that was saved in the arctic-data folder on your server account, make your changes, save the changes, stage them by clicking the box, commit, pull, then push to merge your version with the main version on the website. Do NOT edit scripts using the website. It is much easier to accidentally overwrite the history of a file this way. One thing you might be wondering as you are working on a script is, how often should I be committing my changes? It might not make sense to commit-pull-push after every single tiny change - if only because it would slow you way down. Personally, I commit every time I feel that a significant change has happened and that the chunk of code I was working on is “done”. Sometimes this is an entire script, other times it is just a few lines within a script. A good sign that you are committing too infrequently might be if many of your commit messages address a wide variety of coding tasks, such as: “wrote for loop to create referenced attribute lists for tables 1:20. also created nesting structure for this package with another package. also created attribute list for data table 40.” And one final note, you can make multiple commits before you push to the repo. If you are making lots of changes to the script, you might want to make several commits before pull-push. You can see how many commits you are ahead of the “origin/master” branch (i.e. what you see on the website) by looking for text in your git tab in RStudio that looks like this: 8.3.3 Where do I commit? The default right now is to save data-processing scripts in the arctic-data/datateam/data-processing/ directory, with sub-folders listed by project. Directories can be created as needed but please ask Jesse or Jeanette first so we can try and maintain some semblance of order in the file structure. 8.4 My Git tab disappeared Sometimes R will crash so hard it loses your project information, causing your git tab to disappear. If this happens anything you saved, but did not commit or push in your arctic-data (or sasap-data) folder is no longer being tracked by GitHub. To get the tab back, first, rename your old arctic-data folder to something else (like ‘arctic-data_old’). This will ensure that any work that you had in that folder that you didn’t push to the master branch is not lost. Next, follow the steps above in “Cloning the arctic-data repo” to re-clone the repository. Then, move whatever scripts or parts of scripts that were not being tracked into the arctic-data repo so that they are tracked again, and merge them back into the master branch. Ideally, you are committing and pushing your scripts frequently enough that you don’t have to resort to this. If you had changes you had committed but not pushed, you can still push these changes from the command line. See Jeanette for more info on how to do this if it is the case. Remember: commit, pull, push frequently (at least once a day). "],
["using-rt.html", "Chapter 9 Using RT 9.1 Navigate RT", " Chapter 9 Using RT 9.1 Navigate RT The RT ticketing system is how we communicate with folks interacting with the Arctic Data Center. We use it for managing submissions, accessing issues, etc. It consists of three separate interfaces: Front Page All Tickets Ticket Page 9.1.1 Front page This is what you see first Home - brings you to this homepage Tickets - to search for tickets (also see number 5) Tools - not needed New Ticket - create a new ticket Search - Type in the ticket number to quickly navigate to a ticket Queue - Lists all of the tickets currently in arcticdata and their status New = unopened tickets that require attention Open = tickets currently open, under investigation by team member Stalled = tickets awaiting response from PI/Submitter Tickets I Own - These are the current open tickets that are claimed by me Unowned Tickets - Newest tickets awaiting claim Ticket Status - Status and how long ago it was created Take - claim the ticket as yours 9.1.2 All tickets This is the queue interface from number 6 of the Front page 1. Ticket number and title 2. Ticket status 3. Owner - who has claimed the ticket 9.1.3 Example ticket Title - Include the PI’s name for reference Display - homepage of the ticket History - Comment/Email history, see bottom of Display page Basics - edit the title, status and ownership here People - option to add more people to the watch list for a given ticket conversation. Note that user/ PI/ submitter email addresses should be listed as “Requestors”. Requestors are only emailed on “Replys”, not “Comments”. Ensure your ticket has a Requestor before attempting to contact users/ PIs/ submitters. Links - option to “Merge into” another ticket number if this is part of a larger conversation. Also option to add a reference to other ticket number Actions Reply - message the submitter/ PI/ all watchers Comment - attach internal message (no submitters, only Data Teamers) Open It - Open the ticket Stall - submitter has not responded in greater than 1 month Resolve - ticket completed History - message history and option to reply (to submitter and beyond) or comment (internal message) "],
["introduction-to-solr.html", "Chapter 10 Introduction to Solr 10.1 Querying Solr 10.2 Querying Solr through R 10.3 Key takeaways 10.4 More resources", " Chapter 10 Introduction to Solr Solr is what’s known as an index. More specifically, it’s a piece of software that we install with every instance of Metacat which we use to query all of the Objects Metacat stores (metadata, data, resource maps, etc.). Metacat is our underlying metadata catalog and the foundation of our data repositories for both the CN and all MNs. Every Object in Metacat will have a corresponding Solr document for it that contains information about that Object. Each type of Object will have a different set of fields in its Solr document. For example, an EML document will have a title field (corresponding to the EML document’s &lt;title&gt; element), while a CSV will not. The fields that go into a Solr document are populated by information such as: The System Metadata (fileName, accessPolicy, etc) The Object itself (e.g. title, creators, etc. for an EML record) Additional computed fields (e.g., a geohash for quick spatial search) Indexing the Object’s Metacat stores lets us execute some interesting and very useful queries such as: What are the most recently updated datasets? What Metadata and Data Objects are in a given Data Package? What is the total size (in terms of disk space) of all Objects stored in Metacat? 10.1 Querying Solr Solr is queried via what’s called an HTTP API (Application Programming Interface). Practically, what this means it that you can visit a URL (web address) in a web browser to execute a query. This may be a little bit weird at first but I hope some examples will make it more clear. So I said you visit a URL to query Solr. But what address do you visit? For the Arctic Data Center (https://arcticdata.io), every Solr query starts with a base URL of https://arcticdata.io/metacat/d1/mn/v2/query/solr. If you visit that URL, you will see a list of fields Solr is storing for the Objects it indexes: &lt;ns2:queryEngineDescription xmlns:ns2=&quot;http://ns.dataone.org/service/types/v1.1&quot;&gt; &lt;queryEngineVersion&gt;3.6.2.2012.12.18.19.52.59&lt;/queryEngineVersion&gt; &lt;name&gt;solr&lt;/name&gt; &lt;queryField&gt; &lt;name&gt;abstract&lt;/name&gt; &lt;description&gt; The full text of the abstract as provided in the science metadata document. &lt;/description&gt; ...truncated... You can see that there is a large set of queryable fields, though, as I said above, not all types of Objects will have values set for all of the possible fields because some fields do not make sense for some Objects (e.g., title for a CSV). 10.1.1 Parts of a Query Each Solr query is comprised of a number of parameters. These are like arguments to a function in R, but they are entered as parts of a URL. The most common parameters are: q: The query. This is like subset or dplyr::filter in R fl: What fields are returned for the documents that match your query (q). If not set, all fields are returned. rows: The maximum number of documents to return. Solr will truncate your result if the result size is greater than rows. sort: Sorts the result by the values in the given Solr field (e.g., sort by date uploaded) To use these parameters, we append to the base URL like this: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q={QUERY}&amp;fl={FIELDS}&amp;rows={ROWS} and we replace the text inside {} with the value we want for each parameter. Note that the parameters can come in any order so the following is equivalent: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?fl={FIELDS}&amp;rows={ROWS}&amp;q={QUERY} The first parameter in the URL must have a ‘?’ in front of it and all subsequent parameters must have an ‘&amp;’ between them. It’s really easy to get the URL wrong when typing it in manually like this so be sure to double-check your URL and think critically about the result: Solr tries to always return something even if it’s not what you intended. 10.1.2 Constructing a Query The query (‘q’) parameter uses a syntax that looks like field:value, where field is one of the Solr fields and value is an expression. The expression can match a specific value exactly, e.g., https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=identifier:arctic-data.7747.1 https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=identifier:&quot;doi:10.5065/D60P0X4S&quot; which finds the Solr document for a specific Object by PID (identifier). Note that in the second example, the DOI PID is surrounded in double quotes. This is because Solr has reserved characters, of which ‘:’ is one, so we have to help Solr by surrounding values with reserved characters in them in quotes (as I did here) or escaping them. Queries can take on a more advanced form such as a wildcard expression: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=identifier:arctic-data.* finds all the Objects that start with “arctic-data.” followed by anything (“*“) https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=title:*soil* finds all the Objects with the word “soil” somewhere in the title. https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=origin:*Stafford*+AND+title:Bering Strait&amp;fl=title&amp;sort=title+desc&amp;rows=10 finds 10 Objects where one of the EML creators has a name that contains the substring “Stafford” and the title contains the substring “Bering Strait”, sorted by title (descending order). Note that the +AND+ between the origin and title query above specifies that both conditions must be true for a Solr document to be returned. We could’ve also switched the +AND+ to +OR+ and/or added more conditions to the query. Here’s a slightly more advanced one: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=formatType:METADATA+AND+-obsoletedBy:*&amp;sort=dateUploaded+desc&amp;rows=25 This query is the query MetacatUI uses to fill in the https://arcticdata.io/catalog/ page. Notice the -obsoletedBy:*. The ‘-’ before the field inverts the expression so this part of the query means “things that have no obsoletedBy value set”. We can also just find everything: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=*:* finds any value in any field. 10.1.3 Faceting Above we went through querying across Solr documents but we can also summarize what’s in Solr with Faceting which lets us group Solr documents together and count them. This is like table in R. Faceting can do a query within a query, but more commonly I use it to summarize unique values in a field. For example, we can find the unique format IDs on Data Objects: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=*:*&amp;fq=formatType:DATA&amp;facet=true&amp;facet.field=formatId&amp;rows=0 To facet, we usually do a few things: Add the parameter facet=true Add the parameter facet.field={FIELD} with the field we want to facet (group) on Set rows=0 because we don’t care about the matched Solr documents Optionally specify fq={expression} which filters out Solr documents before faceting. In the above example, we have to do this to only count Data Objects. Without it, the facet result would include formatIDs for metadata and resource maps which we don’t want. 10.1.4 Stats With Faceting, we found we could make queries to find the unique values for a Solr field. With Stats, we can have Solr calculate statistics on numerical values (such as fileSize). https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=formatType:DATA&amp;stats=true&amp;stats.field=size&amp;rows=0 This query calculates a set of summary statistics for the size field on Data Objects that Solr has indexed. In this case, Solr’s size field indexes the fileSize field in the System Metadata for each Object in Metacat. 10.2 Querying Solr through R What if I told you that every time you run the query function in the dataone R package you are asking R to visit a URL like the ones above, parse the information returned by the page, and present it in an R-friendly way such as a list or data.frame? Well that’s what happens! Then why might we use R in the first place? There are two big advantages: The result is returned in a more useful way to R without extra work on your part We can more easily pass our authentication token with the query Why does #2 matter? Well by default, all of those URLs above only returned publicly-readable Solr documents. If a private document matched any of those queries, Solr doesn’t give you any idea and acts like the non-public-readable documents don’t exist. So we must pass an authentication token to access non-public-readable content. This bit is crucial for working with the ADC, so you’ll very often want to use R instead of visiting those URLs in a web browser. And there’s good news: all of the URLs you visited above can be turned into an R expression very easily. For example: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=title:*soil* becomes library(dataone) cn &lt;- CNode(&quot;PROD&quot;) mn &lt;- getMNode(cn, &quot;urn:node:ARCTIC&quot;) # Set your token if you need/want! query(mn, &quot;q=title:*soil*&amp;fl=title&amp;rows=10&quot;) I just deleted the first part of the URL, up to and including the ‘?’, and pasted the rest in as the second argument to query. You may have seen an alternative syntax: query(mn, list(q=&quot;title:*soil*&quot;, fl=&quot;title&quot;, rows=&quot;10&quot;)) this is the same query as above because the query function takes either a string (the first form) or a named list (the second form). By default, query returns the result as a list. This is definitely useful but a data.frame can be a more useful way to work with the result. To get a data.frame instead, just set the as argument to ‘data.frame’ to get a data.frame: query(mn, list(q=&quot;title:*soil*&quot;, fl=&quot;title&quot;, rows=&quot;10&quot;), as = &quot;data.frame&quot;) 1 Daily Average Soil, Air and Ground Temperatures - Council Forest Site [Romanovsky, V.] 2 Canadian Transect of Soils and Vegetation for the Circumpolar Arctic Vegetation Map 3 Soil Temperatures, Toolik Lake, Alaska, 1995 and 1996 4 Soil Temperature ARCSS grid Atqasuk, Alaska 2013 5 X-ray fluorescence, Barrow soils 6 Toolik Lake, Alaska Soil moisture - 2014 7 Thermal Soil Properties for Ivotuk and Council [Beringer, J.] 8 Soil Temperature NIMS grid Barrow, Alaska 2014 9 Ivotuk Soil Data - Station Met2 [Hinzman, L.] 10 thule_tram_soil_temps_2013.csv 10.3 Key takeaways Find out what you can query at https://arcticdata.io/metacat/d1/mn/v2/query/solr The Solr HTTP API is what the R dataone package uses when you run query Pass a token if you want to include non-public-readable objects in the results (you often do!) These skills are highly transferable. Note the syntax of the URL for a Google search. 10.4 More resources Solr’s The Standard Query Parser docs (high level of detail) Another quick reference: https://wiki.apache.org/solr/SolrQuerySyntax http://www.solrtutorial.com/ "],
["nesting-a-data-package.html", "Chapter 11 Nesting a data package 11.1 Introduction 11.2 Create a new parent package 11.3 Add children to an existing parent 11.4 Example", " Chapter 11 Nesting a data package 11.1 Introduction Data packages on member nodes can exist as independent packages or in groups (nested data packages). Much like we can group multiple data files together with a common metadata file, we can group related data packages together with a common “parent” data package. The structure of nested data packages resembles a pyramid. There is one top level, or “parent”, with one or more data packages, or “children”, nested beneath it. There is no limit to how many nested levels can be created, but packages do not generally exceed 3 levels. This “grandparent” has has 5+ children (nested datasets), some of which have children packages of their own. Here are some common uses for nesting: collected data vary by year an NSF award funds several related projects data collection is still ongoing data files exceed the 1000 file limit per data package 11.2 Create a new parent package In some cases, a parent package already exists. Search the ADC for the NSF award number to see if there are already exisiting packages. Parents usually have a UUID rather than a DOI and often start with a title like “Collaborative research:”, but not always. More typically, you will need to create a new parent by editing the existing metadata. The parent package should contain a generalized summary for the metadata of each of its children. To create a new parent, you will need to: Create parent metadata. It’s often easiest to start with a child’s metadata and generalize it Abstract/title: Remove dates and other details that are specific to the child package. Sometimes the NSF award abstract/title will work Data tables/other entities: generally top-level parents do not include data objects, so these sections can be removed Geographic coverage: Expand to include geographic coverage of all children, if needed Temporal coverage: Expand to include temporal ranges of all children, if needed. If the study is ongoing, include the most recent end date; the parent can be updated when additional children are added Methods: Often not needed, but may be included if all children use the same methods Publish the parent metadata to the member node (ADC) using publish_object. Create a resource map to link the parent and children together using create_resource_map and the child_pids argument. Make sure you use the children’s resource map PIDs when you create the resource map! If you forget to, consult Jesse or Jeanette for help on how to fix it. 11.3 Add children to an existing parent Nest the new child using the child_pids argument in publish_update(). rm_child_new &lt;- &quot;some_child_rm_pid&quot; pkg_parent &lt;- get_package(mnTest, rm_parent) rm_parent2 &lt;- publish_update(mnTest, resource_map_pid = rm_parent, metadata_pid = pkg_parent$metadata, child_pids = c(pkg_parent$child_packages, rm_child_new)) # include the resource map PIDs of ALL the children* in the `child_pids` argument, otherwise the nesting relationships between any omitted children and the parent will be deleted Check through all arguments carefully before you publish to production! Do you need to update the metadata? Does the parent include data objects? Does the parent have a parent? Parents can be tricky to fix and work with (especially if they have serial identifiers/SIDs), so if you’re not sure how something works, try it on a test node. 11.4 Example We can start by creating two data packages on the test node to nest beneath a parent. These data packages contain measurements taken from Lake E1 in Alaska in 2013 and 2014. First, load the Arctic Data Center Test Node and libraries. library(dataone) library(arcticdatautils) library(EML) cnTest &lt;- CNode(&#39;STAGING&#39;) mnTest &lt;- getMNode(cnTest,&#39;urn:node:mnTestARCTIC&#39;) We can publish several data files at once using the dir and sapply functions. dir returns the paths to every file located in a folder. dir(&quot;data/2013&quot;, full.names = TRUE) ## [1] &quot;data/2013/2012_2013_winter_E1_temperature.csv&quot; ## [2] &quot;data/2013/2013_summer_E1_temperature.csv&quot; We can use sapply to apply a function over a list or vector. In this instance we will use it to apply publish_object over the vector of files paths (paths) in order to publish them. Then, publish the metadata file to the test node using publish_object. paths &lt;- dir(&quot;/home/dmullen/Nesting_Training/2013&quot;, full.names = TRUE) meta_path &lt;- &quot;/home/dmullen/Nesting_Training/E1_2013metadata.xml&quot; data_pids &lt;- sapply(paths, function(path) { publish_object(mnTest, path, format = &quot;text/csv&quot;) }) metadata_pid &lt;- publish_object(mnTest, path = meta_path, format = format_eml()) Create a resource map to associate the data with the metadata. Here we specify the metadata and data identifiers (PIDs) that we would like the resource map to associate together. rm_2013 &lt;- create_resource_map(mnTest, metadata_pid = metadata_pid, data_pids = data_pids) Repeat the same steps as above for the 2014 data package. # Change paths and meta_path: paths &lt;- dir(&quot;/home/dmullen/Nesting_Training/2014&quot;, full.names = TRUE) meta_path &lt;- &quot;/home/dmullen/Nesting_Training/E1_2014metadata.xml&quot; data_pids &lt;- sapply(paths, function(path) { publish_object(mnTest, path, format = &quot;text/csv&quot;) }) metadata_pid &lt;- publish_object(mnTest, path = meta_path, format = format_eml()) # Store new resource map as rm_2014 rm_2014 &lt;- create_resource_map(mnTest, metadata_pid = metadata_pid, data_pids = data_pids) These two packages correspond to data from the same study, varying only by year; however, they currently exist on the test node as independent entities. We will associate them with each other by nesting them underneath a parent. Now, let’s create a parent metadata file. Read in one of the children’s metadata files (EML). eml_parent &lt;- read_eml(&quot;data/E1_2013metadata.xml&quot;) ## Use this code when you are reading in the parent # eml_parent &lt;- read_eml(&quot;/home/dmullen/Nesting_Training/E1_2013metadata.xml&quot;) ## View the title eml_parent@dataset@title ## An object of class &quot;ListOftitle&quot; ## [[1]] ## &lt;title&gt;Time series of water temperature, specific conductance and oxygen from Lake E1, North Slope, Alaska, 2012-2013.&lt;/title&gt; The title of this child contains “2012-2013”. This is too specific for the parent, as the temporal range of both children is 2012-2014. The parent should encompass this larger time range. new_title &lt;- &quot;Time series of water temperature, specific conductance, and oxygen from Lake E1, North Slope, Alaska, 2012-2014&quot; eml_parent@dataset@title &lt;- c(new(&quot;title&quot;, .Data = new_title)) Like the title, the temporal coverage elements in this EML need to be adjusted. new_end_date &lt;- new(&quot;calendarDate&quot;, &quot;2014-09-20&quot;) eml_parent@dataset@coverage@temporalCoverage@.Data[[1]]@rangeOfDates@endDate@calendarDate &lt;- new_end_date Remove dataTables and otherEntitys from the metadata. If you recall from previous chapters, dataTables contain metadata associated with data files (generally CSVs) and otherEntitys contain metadata about any other files in the data package (for instance a README or coding script). Because the parent does not contain any data objects, we want to remove dataTables and otherEntities from the metdata file. In this instance, the E1 2013 metadata only contains dataTables. We can remove these by setting the dataTable element in the EML to a new blank object. eml_parent@dataset@dataTable &lt;- new(&quot;ListOfdataTable&quot;) In this case, the abstract, contacts, creators, geographic description, and methods are already generalized and do not require changes. Before writing your parent EML make sure that it validates. This is just a check to make sure everything is in the correct format. eml_validate(eml_parent) After your EML validates we need to save, or “write”, it as a new file. Write your parent EML to a directory in your home folder. You can view this process like using “Save as” in Microsoft Word. We opened a file (E1_2013.xml), made some changes, and “saved it as” a new file called eml_parent.xml. ## This will create the file &quot;eml_parent.xml&quot; at the location specified by path path = &quot;/home/YOURUSERNAME/eml_parent.xml&quot; write_eml(eml_parent, path) Next, we will publish the parent metadata to the test node. metadata_parent &lt;- publish_object(mnTest, path = path, format_id = format_eml()) Finally, we create a resource map for the parent package. We nest the two child data packages using the child_pids argument in create_resource_map. Note that these child_pids are pids for the resource maps of the child packages, NOT the metadata pids. resourceMap_parent &lt;- create_resource_map(mnTest, metadata_pid = metadata_parent, child_pids = c(rm_2013, rm_2014)) The child packages are now nested underneath the parent. "],
["building-provenance.html", "Chapter 12 Building Provenance 12.1 Introduction 12.2 The Prov Editor 12.3 Understanding resource maps 12.4 datapack 12.5 References", " Chapter 12 Building Provenance 12.1 Introduction The provenance chain describes the origin and processing history of data. Provenance can exist on a continuum, ranging from prose descriptions of the history, to formal provenance traces, to fully executable environments. In this section we will describe how to build provenance using formal provenance traces in DataONE. Provenance is becoming increasingly important in the face of what is being called a reproducibility crisis in science. J. P. A. Ioannidis (2005) wrote that “Most Research Findings Are False for Most Research Designs and for Most Fields”. Ioannidis outlined ways in which the research process has lead to inflated effect sizes and hypothesis tests that codify existing biases. The first step towards addressing these issues is to be able to evaluate the data, analyses, and models on which conclusions are drawn. Under current practice, this can be difficult because data are typically unavailable, the method sections of papers do not detail the computational approaches used, and analyses and models are often conducted in graphical programs, or, when scripted analyses are employed, the code is not available. And yet, this is easily remedied. Researchers can achieve computational reproducibility through open science approaches, including straightforward steps for archiving data and code openly along with the scientific workflows describing the provenance of scientific results (e.g., Hampton et al. (2015), Munafò et al. (2017)). At NCEAS and in the datateam, not only do we archive data and code openly, but we also describe the workflows that involve that data and code using provenance, formalizing the provenance trace for a workflow that might look like this into an easily understandable trace including archived data objects, such as what is shown here. There are two ways that we add provenance in the datateam - the prov editor and the R datapack package. 12.2 The Prov Editor Provenance can easily be added to production Arctic Data Center packages using the provenance editor on beta.arcticdata.io. On the landing page of a data package within beta, in the dataTable or otherEntity section where you would like to add a provenance relationship, you can choose to add either a “source” or a “derivation”, to the left or right of the object pane, respectively. add prov To add a source data file, click on the circle with the “+ add” text. Similarly, a source script would be added by selecting the arrow. Selecting the circle to add a source file pulls up the following screen, where you can select the source from other data objects within the same data package. A data package with an object that has multiple sources added will look like this. For simple packages on the Arctic Data Center, adding prov through the prov editor at beta.arcticdata.io is super easy! 12.3 Understanding resource maps Before we dive further into constructing prov in R, we need to talk more about resource maps. All Data Packages have a single Resource Map. But what is a Resource Map and how do we use one to find out what Objects are in a particular Data Package? This document is a short introduction but a more complete guide can be found here. A Resource Map is a special kind of XML document that describes (amongst other things) an Aggregation. The Aggregation describes the members of a Data Package (metadata and data, usually). We can use the dataone R package to download a Resource Map if we know its PID: library(dataone) mn &lt;- MNode(&quot;https://test.arcticdata.io/metacat/d1/mn/v2&quot;) pid &lt;- &quot;urn:uuid:82bd7d7f-9e18-4fd2-8bda-99b1fddab556&quot; # A Resource Map PID path &lt;- tempfile(fileext = &quot;.xml&quot;) # We&#39;re saving to a temporary file but you can save elsewhere writeLines(rawToChar(getObject(mn, pid)), path) # Write the object to `path` If we open that file up on a text editor, we see this: &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt; &lt;rdf:RDF xmlns:cito=&quot;http://purl.org/spar/cito/&quot; xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:dcterms=&quot;http://purl.org/dc/terms/&quot; xmlns:foaf=&quot;http://xmlns.com/foaf/0.1/&quot; xmlns:ore=&quot;http://www.openarchives.org/ore/terms/&quot; xmlns:prov=&quot;http://www.w3.org/ns/prov#&quot; xmlns:provone=&quot;http://purl.dataone.org/provone/2015/01/15/ontology#&quot; xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot; xmlns:rdfs=&quot;http://www.w3.org/2000/01/rdf-schema#&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema#&quot;&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;&gt; &lt;cito:isDocumentedBy rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;rdf:type rdf:resource=&quot;http://www.openarchives.org/ore/terms/Aggregation&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;cito:isDocumentedBy rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;cito:documents rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;cito:documents rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;&gt; &lt;ore:isAggregatedBy rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;dc:title&gt;DataONE Aggregation&lt;/dc:title&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556&quot;&gt; &lt;dcterms:identifier rdf:datatype=&quot;http://www.w3.org/2001/XMLSchema#string&quot;&gt;urn:uuid:82bd7d7f-9e18-4fd2-8bda-99b1fddab556&lt;/dcterms:identifier&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556&quot;&gt; &lt;ore:describes rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;ore:isAggregatedBy rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;ore:aggregates rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;ore:aggregates rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556&quot;&gt; &lt;rdf:type rdf:resource=&quot;http://www.openarchives.org/ore/terms/ResourceMap&quot;/&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;&gt; &lt;dcterms:identifier rdf:datatype=&quot;http://www.w3.org/2001/XMLSchema#string&quot;&gt;urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27&lt;/dcterms:identifier&gt; &lt;/rdf:Description&gt; &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;&gt; &lt;dcterms:identifier rdf:datatype=&quot;http://www.w3.org/2001/XMLSchema#string&quot;&gt;urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5&lt;/dcterms:identifier&gt; &lt;/rdf:Description&gt; &lt;/rdf:RDF&gt; Whoa. What is this thing and how do you read it? The short of it is that, if you want to find the members of the Data Package, you want to look for lines like this: &lt;rdf:Description rdf:about=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3A82bd7d7f-9e18-4fd2-8bda-99b1fddab556#aggregation&quot;&gt; &lt;ore:aggregates rdf:resource=&quot;https://cn.dataone.org/cn/v2/resolve/urn%3Auuid%3Ac59b7505-39e6-4def-bc82-b67a8d117ce5&quot;/&gt; This line says “The Aggregation aggregates urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5” so that means urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5 is in our Data Package! The key bit is the &lt;rdf:Description rdf:about=&quot;...#aggregation part. If you look for another similar statement, you’ll also see that urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27 is part of our Data Package. Now we know which Objects are in our Data Package but we don’t know which one is metadata and which one is data. For that, we need to get a copy of the System Metadata for each object: library(dataone) mn &lt;- MNode(&quot;https://test.arcticdata.io/metacat/d1/mn/v2&quot;) getSystemMetadata(mn, &quot;urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5&quot;)@formatId [1] &quot;eml://ecoinformatics.org/eml-2.1.1&quot; getSystemMetadata(mn, &quot;urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27&quot;)@formatId [1] &quot;application/octet-stream&quot; From the format IDs, we can see the first PID is metadata and the second PID is data. Now we know enough to know what’s in the Data Package: Resource Map: urn:uuid:82bd7d7f-9e18-4fd2-8bda-99b1fddab556 Metadata: urn:uuid:c59b7505-39e6-4def-bc82-b67a8d117ce5 Data: urn:uuid:61b48e72-ea29-4ba5-8131-4f59a9ebcd27 Now that you’ve actually seen a resource map, we can dive further into prov. 12.4 datapack For packages not on the ADC, or packages that are extremely complicated, it may be best to upload prov relationships using R. The datapack package has several functions which help add relationships in a very simple way. These relationships are stored in the resource map. When you update a package just by adding prov, the package will not get any new identifiers with the exception of the resource map. First, we set the environment, in a similar, but slightly different way than what you may be used to. Here the function D1Client sets the DataONE client with the coordinating node instance as the first argument, and membernode as the second argument. library(dataone) library(datapack) d1c &lt;- D1Client(&quot;STAGING2&quot;, &quot;urn:node:mnTestKNB&quot;) Next, get the pid of the resource map of the data package you are adding prov to, and load that package into R using the getDataPackage function. resmapId &lt;- &quot;urn:uuid:8f501606-2c13-4454-b22d-050a4176a97b&quot; pkg &lt;- getDataPackage(d1c, id=resmapId, lazyLoad=TRUE, limit=&quot;0MB&quot;, quiet=FALSE) Printing pkg in your console shows you the contents of the data package, including all of the objects and their names: &gt; pkg Members: filename format mediaType size identifier modified local esc...er.R application/R NA 888 knb.92049.1 n n PWS....csv text/csv NA 1871469 knb.92050.1 n n PWS....csv text/csv NA 1508128 knb.92051.1 n n NA eml:/...-2.1.1 NA 15658 urn:uuid:8f501606-2c13-4454-b22d-050a4176a97b n y Package identifier: resource_map_urn:uuid:8f501606-2c13-4454-b22d-050a4176a97b RightsHolder: http://orcid.org/0000-0002-2192-403X It will also show the existing relationships in the resource map, which in this case are mostly the “documents” relationships that specify that the metadata record is describing all of these data files. Relationships: subject predicate object 2 esc_reformatting_PWSweirTower.R cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b 4 PWS_weirTower.csv cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b 1 PWS_Weir_Tower_export.csv cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b 3 urn:uuid:8f501606-...4-b22d-050a4176a97b dcterms:creator _r1515542097r415842r1 5 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:documents esc_reformatting_PWSweirTower.R 6 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:documents PWS_weirTower.csv 7 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:documents PWS_Weir_Tower_export.csv 8 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:documents urn:uuid:8f501606-...4-b22d-050a4176a97b 9 urn:uuid:8f501606-...4-b22d-050a4176a97b cito:isDocumentedBy urn:uuid:8f501606-...4-b22d-050a4176a97b In this example above, the data package has two .csv files, with an R script that converts one to the other. To create our provenance trace, first we need to select the source object, and save the pid to a variable. We do this using the selectMember function, and we can query part of the system metadata to select the file that we want. This function takes the data package (pkg), the name of the sysmeta field to query (in this case we use the fileName), and the value that you want to match that field to (in this case, ‘PWS_Weir_Tower_export.csv’). sourceObjId &lt;- selectMember(pkg, name=&quot;sysmeta@fileName&quot;, value=&#39;PWS_Weir_Tower_export.csv&#39;) This returns a list of the source object pids that match the query (in this case only one object matches). &gt; sourceObjId [1] &quot;knb.92051.1&quot; Now we need to select our output object. Here, we use the selectMember function again, and save the result to a new variable. outputObjId &lt;- selectMember(pkg, name=&quot;sysmeta@fileName&quot;, value=&#39;PWS_weirTower.csv&#39;) Now we query for the R script. In this case, we query based on the value of the formatId as opposed to the filename. This can be useful if you wish to select a large list of pids that are all similar. programObjId &lt;- selectMember(pkg, name=&quot;sysmeta@formatId&quot;, value=&quot;application/R&quot;) Next, you use these lists of pids and a function called describeWorkflow to add these relationships to the data package. Note that if you do not have a program in the workflow, or a source file, you can simply leave those arguments blank. pkg &lt;- describeWorkflow(pkg, sources=sourceObjId, program=programObjId, derivations=outputObjId) Viewing pkg again confirms that these relationships have been inserted into the data package, as shown by the “wasDerivedFrom” and “wasGeneratedBy” statements. It is always a good idea to print pkg to confirm that your pid selection process worked as expected, and your prov relationships make sense. Relationships (updated): subject predicate object 15 _1db49d06-ae98-4...9101-39f7c0b45a95 rdf:type prov:Association 14 _1db49d06-ae98-4...9101-39f7c0b45a95 prov:hadPlan esc_reformatting_PWSweirTower.R 1 esc_reformatting_PWSweirTower.R cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b 16 esc_reformatting_PWSweirTower.R rdf:type provone:Program 8 PWS_weirTower.csv cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b 11 PWS_weirTower.csv rdf:type provone:Data 20 PWS_weirTower.csv prov:wasDerivedFrom PWS_Weir_Tower_export.csv 19 PWS_weirTower.csv prov:wasGeneratedBy urn:uuid:3dd59b0...bc38-3b5d8fa644ac 6 PWS_Weir_Tower_export.csv cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b 10 PWS_Weir_Tower_export.csv rdf:type provone:Data 9 _r1515544826r415842r1 foaf:name DataONE R Client 17 urn:uuid:3dd59b0...bc38-3b5d8fa644ac dcterms:identifier urn:uuid:3dd59b0...bc38-3b5d8fa644ac 13 urn:uuid:3dd59b0...bc38-3b5d8fa644ac rdf:type provone:Execution 12 urn:uuid:3dd59b0...bc38-3b5d8fa644ac prov:qualifiedAssociation _1db49d06-ae98-4...9101-39f7c0b45a95 18 urn:uuid:3dd59b0...bc38-3b5d8fa644ac prov:used PWS_Weir_Tower_export.csv 5 urn:uuid:8f50160...b22d-050a4176a97b cito:documents esc_reformatting_PWSweirTower.R 4 urn:uuid:8f50160...b22d-050a4176a97b cito:documents PWS_weirTower.csv 3 urn:uuid:8f50160...b22d-050a4176a97b cito:documents PWS_Weir_Tower_export.csv 2 urn:uuid:8f50160...b22d-050a4176a97b cito:documents urn:uuid:8f50160...b22d-050a4176a97b 7 urn:uuid:8f50160...b22d-050a4176a97b cito:isDocumentedBy urn:uuid:8f50160...b22d-050a4176a97b Finally, you can upload the data package using the uploadDataPackage function, which takes the DataONE client d1c we set in the beginning, the updated pkg variable, and some options for public read and whether informational messages are printed during the upload process. resmapId_new &lt;- uploadDataPackage(d1c, pkg, public=TRUE, quiet=FALSE) If successful you should be able to navigate to the landing page of your dataset, and icons should show up where the sources and derivations are, such as in this example 12.4.1 Fixing mistakes If you messed up updating a datapackage using datapack, there unfortunately isn’t a great way to undo your work, as the describeWorkflow only adds prov relationships, it does not replace them. If you messed up, the best course of action is to update the resource map with a clean version that does not have prov using update_resource_map, and then go through the steps outlined above again. Note: this has not been thorougly tested, and more extreme actions may be necessary to fully nuke the prov relationships. See Jeanette if things do not work as expected. 12.5 References Ioannidis, John P A. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124. doi:10.1371/journal.pmed.0020124. Hampton, Stephanie E, Sean Anderson, Sarah C Bagby, Corinna Gries, Xueying Han, Edmund Hart, Matthew B Jones, et al. 2015. “The Tao of Open Science for Ecology.” Ecosphere 6 (July). doi:http://dx.doi.org/10.1890/ES14-00402.1. Munafò, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop, Katherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A. Ioannidis. 2017. “A Manifesto for Reproducible Science.” Nature Human Behaviour 1 (1): 0021. doi:10.1038/s41562-016-0021. "]
]
